{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4e69fb",
   "metadata": {},
   "source": [
    "Change default model caching directory from `~/.cache` to the default work directory in the cluster meant for big files like model weights etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f661ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/work/mremeli/huggingface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc88702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mremeli/mambaforge/envs/DL_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b78e0",
   "metadata": {},
   "source": [
    "## Load LLM of choice\n",
    "\n",
    "To retrieve the probabilities of generated sequences we have to load a causal language model architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b6142c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 665/665 [00:00<00:00, 197kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 548M/548M [00:09<00:00, 57.5MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 1.95MB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 867kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 2.11MB/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"gpt2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, return_dict_in_generate=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb4a40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"return_dict_in_generate\": true,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.24.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d2a1ae",
   "metadata": {},
   "source": [
    "### Input prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22fdb84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[22253,  2266, 10311, 14263,  1816,   284,   262,  8222,   530,  1110,\n",
      "           284]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "prompt = [\"Little red riding hood went to the forest one day to\"]\n",
    "tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(tokenized_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d431a",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528eb5b",
   "metadata": {},
   "source": [
    "Set the desired maximum number of generated tokens (we set it to 40)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f61f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens_generated = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b13517b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**tokenized_prompt, \n",
    "                         output_scores=True, # output logits for each newly generated word\n",
    "                         max_new_tokens=max_tokens_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9e68823",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GreedySearchDecoderOnlyOutput(sequences=tensor([[22253,  2266, 10311, 14263,  1816,   284,   262,  8222,   530,  1110,\n",
       "           284,   766,   611,   339,   714,  1064,   257,   835,   284,   651,\n",
       "           503,    13,   679,  1043,   257,  1402, 40812,   287,   262,  8222,\n",
       "           290,  1816,   284,   340,    13,   679,  1043,   257,  1402, 40812,\n",
       "           287,   262,  8222,   290,  1816,   284,   340,    13,   679,  1043,\n",
       "           257]]), scores=(tensor([[-131.5153, -131.9083, -137.2242,  ..., -137.5335, -135.5820,\n",
       "         -133.1908]]), tensor([[-107.9540, -109.3710, -113.5140,  ..., -115.1297, -114.9533,\n",
       "         -111.1234]]), tensor([[-101.0773, -101.5922, -106.0210,  ..., -104.5540, -106.1922,\n",
       "         -102.7193]]), tensor([[-133.3365, -133.3741, -140.1894,  ..., -141.9558, -133.4405,\n",
       "         -135.5265]]), tensor([[-138.2824, -140.4560, -145.9526,  ..., -148.9709, -147.9707,\n",
       "         -141.7005]]), tensor([[-104.0010, -104.2025, -110.0863,  ..., -107.3719, -110.4027,\n",
       "         -105.2797]]), tensor([[-115.9442, -114.8154, -119.0849,  ..., -120.0303, -118.8562,\n",
       "         -115.5782]]), tensor([[ -95.9490,  -97.7325, -105.0876,  ..., -108.2207, -107.5619,\n",
       "         -100.0494]]), tensor([[-140.8422, -141.4425, -147.1155,  ..., -148.6548, -149.2324,\n",
       "         -143.0613]]), tensor([[-136.2022, -136.2788, -141.9907,  ..., -141.1356, -140.9731,\n",
       "         -138.0889]]), tensor([[-86.5142, -88.0368, -94.0763,  ..., -98.7684, -99.3065, -90.5349]]), tensor([[-155.6450, -153.7933, -156.8721,  ..., -167.5396, -167.4514,\n",
       "         -149.0872]]), tensor([[-154.7857, -155.0755, -160.1605,  ..., -163.9464, -152.3161,\n",
       "         -156.3727]]), tensor([[-101.0617, -101.7121, -106.7512,  ..., -107.2447, -108.6658,\n",
       "         -103.7873]]), tensor([[-110.9850, -109.6784, -113.1473,  ..., -116.1323, -113.9862,\n",
       "         -111.4520]]), tensor([[-100.6840,  -98.5623, -103.6248,  ..., -105.5276, -105.2607,\n",
       "         -101.8899]]), tensor([[-117.3930, -118.6740, -124.1955,  ..., -128.8083, -125.1121,\n",
       "         -121.2644]]), tensor([[-69.3351, -68.0978, -71.9521,  ..., -73.5064, -73.8478, -70.3247]]), tensor([[-110.0401, -108.2521, -111.7014,  ..., -110.4047, -113.6169,\n",
       "         -109.4853]]), tensor([[-102.3242, -103.3433, -109.8111,  ..., -111.5560, -109.6280,\n",
       "         -105.4385]]), tensor([[-157.4160, -158.1327, -164.3669,  ..., -163.6086, -155.3400,\n",
       "         -159.0532]]), tensor([[-135.3423, -137.6354, -145.1638,  ..., -145.5588, -143.0160,\n",
       "         -139.5454]]), tensor([[-113.6326, -114.0235, -119.2590,  ..., -118.6083, -117.9249,\n",
       "         -115.0838]]), tensor([[ -98.3811, -101.1894, -108.6738,  ..., -111.7643, -106.0971,\n",
       "         -103.0988]]), tensor([[-154.7548, -153.3586, -156.4936,  ..., -166.5600, -164.1429,\n",
       "         -148.6631]]), tensor([[-159.1960, -159.6030, -166.1440,  ..., -168.8953, -156.6227,\n",
       "         -160.7519]]), tensor([[ -96.7848,  -96.4620, -102.2849,  ..., -103.0738, -102.9114,\n",
       "          -98.7175]]), tensor([[-109.8842, -108.2719, -112.3038,  ..., -115.4794, -112.6604,\n",
       "         -110.2268]]), tensor([[ -97.9789,  -96.3670, -102.1046,  ..., -103.7808, -102.9461,\n",
       "          -99.2438]]), tensor([[-104.3321, -105.7071, -111.9657,  ..., -117.6513, -113.3573,\n",
       "         -108.4149]]), tensor([[-14.2992, -13.4406, -17.9929,  ..., -20.2513, -19.3951, -15.2938]]), tensor([[-84.0523, -82.5378, -86.3548,  ..., -85.8206, -88.0151, -83.6993]]), tensor([[ -89.3934,  -90.3889,  -97.8484,  ..., -100.9509,  -98.0448,\n",
       "          -92.3130]]), tensor([[-124.7309, -126.4062, -133.0908,  ..., -133.5679, -124.3038,\n",
       "         -126.2032]]), tensor([[-26.1195, -27.6114, -33.2291,  ..., -36.7809, -35.1862, -29.1815]]), tensor([[-26.9641, -26.8878, -32.6303,  ..., -36.0546, -36.5568, -28.5587]]), tensor([[  0.4970,  -0.1659,  -5.8750,  ..., -13.2073,  -9.8928,  -1.7202]]), tensor([[-137.4010, -137.0073, -138.9561,  ..., -146.0421, -145.1513,\n",
       "         -130.6515]]), tensor([[-116.8052, -117.3703, -122.8668,  ..., -125.4801, -116.0842,\n",
       "         -116.8599]]), tensor([[-28.7652, -27.2892, -31.8799,  ..., -35.0109, -35.3733, -28.0090]])), attentions=None, hidden_states=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ea294",
   "metadata": {},
   "source": [
    "The output has two parts. One is the generated sequences, and the other the logits (or scores) that the model calculated for each new generated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d29924ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'scores'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd090982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Little red riding hood went to the forest one day to see if he could find a way to get out. He found a small hut in the forest and went to it. He found a small hut in the forest and went to it. He found a\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.batch_decode(outputs.sequences)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de82cecd",
   "metadata": {},
   "source": [
    "#### Output probabilities\n",
    "These scores can be converted to probabilities using the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3c43595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[4.6492e-06, 3.1384e-06, 1.5419e-08,  ..., 1.1317e-08,\n",
      "          7.9661e-08, 8.7047e-07],\n",
      "         [1.1294e-04, 2.7379e-05, 4.3468e-07,  ..., 8.6389e-08,\n",
      "          1.0305e-07, 4.7463e-06],\n",
      "         [7.9526e-06, 4.7522e-06, 5.6690e-08,  ..., 2.4580e-07,\n",
      "          4.7771e-08, 1.5397e-06],\n",
      "         ...,\n",
      "         [2.9504e-06, 4.3737e-06, 6.2299e-07,  ..., 5.2129e-10,\n",
      "          1.2705e-09, 2.5185e-03],\n",
      "         [3.1661e-06, 1.7992e-06, 7.3791e-09,  ..., 5.4081e-10,\n",
      "          6.5105e-06, 2.9975e-06],\n",
      "         [1.0764e-05, 4.7095e-05, 4.7782e-07,  ..., 2.0868e-08,\n",
      "          1.4524e-08, 2.2927e-05]]])\n"
     ]
    }
   ],
   "source": [
    "probs = torch.stack(outputs.scores, dim=1).softmax(-1) \n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352fbfb",
   "metadata": {},
   "source": [
    "Shape: \\[batch_size, num_generated_tokens, vocab_size\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26deae9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 50257])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10000e51",
   "metadata": {},
   "source": [
    "The first generated word is 'see'. Let's see (pun intended) what the assigned probability was!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24f7488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior:\t 'Little red riding hood went to the forest one day to'\n",
      "Probability of next word ' see' is: 6.42%\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "prompt_len = tokenized_prompt.input_ids.shape[1]\n",
    "word = tokenizer.decode(outputs.sequences[:,prompt_len+idx])\n",
    "\n",
    "word_prob = torch.max(probs[:,idx,:])\n",
    "print(\"Prior:\\t '%s'\" % ' '.join(generated_text.split()[:prompt_len+idx]))\n",
    "print(\"Probability of next word '%s' is: %.2f%%\" % (word, word_prob*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f8979-3554-439b-a80a-4ed950c99e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
