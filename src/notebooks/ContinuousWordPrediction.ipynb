{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592123ae",
   "metadata": {},
   "source": [
    "Change default model caching directory from `~/.cache` to the default work directory in the cluster meant for big files like model weights etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23af60f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mremeli/mambaforge/envs/DL_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/work/mremeli/huggingface'\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc7f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff653a1",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3bce59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a2d89",
   "metadata": {},
   "source": [
    "## Input\n",
    "The input will be the first few lines of a Dr Seuss kids story called 'Green eggs and ham'.\n",
    "This is a nice example because it uses relatively few tokens, is quite repetitive, so the language model should be able to predict the next few tokens accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448d1f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_text = \"\"\"\n",
    "I AM SAM. I AM SAM. SAM I AM.\n",
    "\n",
    "THAT SAM-I-AM! THAT SAM-I-AM! I DO NOT LIKE THAT SAM-I-AM!\n",
    "\n",
    "DO WOULD YOU LIKE GREEN EGGS AND HAM?\n",
    "\n",
    "I DO NOT LIKE THEM,SAM-I-AM.\n",
    "I DO NOT LIKE GREEN EGGS AND HAM.\n",
    "\n",
    "WOULD YOU LIKE THEM HERE OR THERE?\n",
    "\n",
    "I WOULD NOT LIKE THEM HERE OR THERE.\n",
    "I WOULD NOT LIKE THEM ANYWHERE.\n",
    "I DO NOT LIKE GREEN EGGS AND HAM.\n",
    "I DO NOT LIKE THEM, SAM-I-AM.\n",
    "\n",
    "WOULD YOU LIKE THEM IN A HOUSE?\n",
    "WOULD YOU LIKE THEN WITH A MOUSE?\n",
    "\n",
    "I DO NOT LIKE THEM IN A HOUSE.\n",
    "I DO NOT LIKE THEM WITH A MOUSE.\n",
    "I DO NOT LIKE THEM HERE OR THERE.\n",
    "I DO NOT LIKE THEM ANYWHERE.\n",
    "I DO NOT LIKE GREEN EGGS AND HAM.\n",
    "I DO NOT LIKE THEM, SAM-I-AM.\n",
    "\n",
    "WOULD YOU EAT THEM IN A BOX?\n",
    "WOULD YOU EAT THEM WITH A FOX?\n",
    "\n",
    "NOT IN A BOX. NOT WITH A FOX.\n",
    "NOT IN A HOUSE. NOT WITH A MOUSE.\n",
    "I WOULD NOT EAT THEM HERE OR THERE.\n",
    "I WOULD NOT EAT THEM ANYWHERE.\n",
    "I WOULD NOT EAT GREEN EGGS AND HAM.\n",
    "I DO NOT LIKE THEM, SAM-I-AM.\n",
    "\n",
    "WOULD YOU? COULD YOU? IN A CAR?\n",
    "EAT THEM! EAT THEM! HERE THEY ARE.\n",
    "\n",
    "I WOULD NOT, COULD NOT, IN A CAR.\n",
    "\n",
    "YOU MAY LIKE THEM. YOU WILL SEE.\n",
    "YOU MAY LIKE THEM IN A TREE!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "future_text = \"\"\"I WOULD NOT, COULD NOT IN A TREE.\n",
    "NOT IN A CAR! YOU LET ME BE.\n",
    "I DO NOT LIKE THEM IN A BOX.\n",
    "I DO NOT LIKE THEM WITH A FOX.\n",
    "I DO NOT LIKE THEM IN A HOUSE.\n",
    "I DO NOT LIKE THEM WITH A MOUSE.\n",
    "I DO NOT LIKE THEM HERE OR THERE.\n",
    "I DO NOT LIKE THEM ANYWHERE.\n",
    "I DO NOT LIKE GREEN EGGS AND HAM.\n",
    "I DO NOT LIKE THEM, SAM-I-AM.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59adbe46",
   "metadata": {},
   "source": [
    "Next, we tokenize the 'past' text which will serve as a prior for our generated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2054f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_prompt = tokenizer(past_text, return_tensors=\"pt\")\n",
    "num_past_tokens = tokenized_prompt.input_ids.shape[1]\n",
    "outputs = model.generate(**tokenized_prompt, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9f1d4",
   "metadata": {},
   "source": [
    "Then, we extract the generated text from the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d35a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I WOULD NOT, COULD NOT, IN A TREE.\n",
      "\n",
      "YOU MAY LIKE\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.batch_decode(outputs[:,num_past_tokens:])[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b2e07",
   "metadata": {},
   "source": [
    "As we can see, the next line was accurately predicted. --> 'I WOULD NOT, COULD NOT, IN A TREE.'\n",
    "\n",
    "\n",
    "Afterwards the rhyme does not follow the true text. --> ~~'YOU MAY LIKE'~~ 'NOT IN A CAR! YOU LET ME BE.'\n",
    "\n",
    "\n",
    "Not bad nevertheless!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c792e",
   "metadata": {},
   "source": [
    "## Continuous next word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2f3cc",
   "metadata": {},
   "source": [
    "There are X past tokens and Y future tokens.\n",
    "\n",
    "So far, our task was the following: `pred(tokens[:X]) ?= tokens[X:]`. Predict the future tokens based on past tokens.\n",
    "\n",
    "Next, we would only like to predict the next word accurately.\n",
    "\n",
    "Task: \n",
    "```\n",
    "for k=0...Y\n",
    "    pred(tokens[:X+k]) ?= tokens[X+k+1]\n",
    "    \n",
    "```\n",
    "\n",
    "We would like to see how frequently we are able to predict the next word, and express that in the percentage of correctly predicted words (correct/Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18df5e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_token_dict(token_dict, max_idx):\n",
    "    new_dict = {}\n",
    "    for key, val in token_dict.items():\n",
    "        new_dict[key] = val[:,:max_idx]\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab72f13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 112/112 [01:17<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total_preds = 0\n",
    "\n",
    "full_text = past_text + future_text\n",
    "full_tokenized = tokenizer(full_text, return_tensors=\"pt\") # tokenize full text\n",
    "num_tokens = full_tokenized.input_ids.shape[1]\n",
    "\n",
    "for k in tqdm(range(num_tokens - num_past_tokens)):\n",
    "    num_used_tokens = k + num_past_tokens\n",
    "    partial_tokenized = slice_token_dict(full_tokenized, num_used_tokens)\n",
    "    outputs = model.generate(**partial_tokenized, max_new_tokens=1) # predict only next word\n",
    "    \n",
    "    predicted_token = outputs[:,-1].item()\n",
    "    true_token = full_tokenized.input_ids[:,num_used_tokens]\n",
    "    \n",
    "    total_preds += 1\n",
    "    if predicted_token == true_token:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "514ae2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num correct: 87\n",
      "Total preds: 112\n",
      "% of correctly predicted words: 77.68%\n"
     ]
    }
   ],
   "source": [
    "print(\"Num correct: %d\" % correct)\n",
    "print(\"Total preds: %d\" % total_preds)\n",
    "print(\"%% of correctly predicted words: %.2f%%\" % (100*(correct/total_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740348eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
